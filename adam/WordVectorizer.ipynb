{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamr\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "from numba import jit, cuda\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_remover(df):\n",
    "    df['text'] = df['text'].astype(str).str.lower()\n",
    "    df['token_text'] = df['text'].str.replace('([^ a-zA-Z0-9])', '').str.replace('http\\S+|www.\\S+', '', case=False).replace('coronavirus', 'covid19')\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    df['stop_text'] = df['token_text'].apply(lambda x: [item for item in str(x).split() \n",
    "                                                    if item not in stop])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../adam/datasets/scrape_5.12.csv')\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121482, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114044, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.drop_duplicates(inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = stop_word_remover(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('../adam/datasets/test_5.14.csv')\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221214, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201794, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop_duplicates(inplace=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = stop_word_remover(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315838, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1,df2], ignore_index = True, verify_integrity = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "df['token_text'] = df['text'].str.replace('([^ a-zA-Z0-9])', '').str.replace('http\\S+|www.\\S+', '', case=False).replace('coronavirus', 'covid19')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "df['stop_text'] = df['token_text'].apply(lambda x: [item for item in str(x).split() \n",
    "                                                    if item not in stop])\n",
    "\n",
    "\n",
    "\n",
    "#df['split_text'] = df['token_text'].astype(str).str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    did the first of several shopping runs, though i ended up walking out of cvs without buying anything. if the rate of people coughing at me is any indication, we’ll all have covid-19 by thursday. \\n\\ncover your fucking mouths when you cough.\n",
       "1    looks like adios muchachos for the us #covid #covid19us #coronavirushttps://twitter.com/nytimes/status/1234171621696557057 …                                                                                                                    \n",
       "2    humour..snl...on #covid #coronavirususahttps://mashable.com/video/snl-coronavirus-democratic-candidates-cold-open/ …                                                                                                                            \n",
       "3    had to go to three diff convenient stores to find real milk (not soy) and they were all out of the lower priced non organic stuff .. i buy organic so i was fine with it but wow. covid 19 panic buying begins in manhattan.                    \n",
       "4    #covid #coronovirius in nursing home setting , #kirkland more than 50 with symptoms ..3/1/20https://www.statnews.com/2020/02/29/new-covid-19-death-raises-concerns-about-virus-spread-in-nursing-homes/ …                                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [first, several, shopping, runs, though, ended, walking, cvs, without, buying, anything, rate, people, coughing, indication, well, covid19, thursday, cover, fucking, mouths, cough]\n",
       "1    [looks, like, adios, muchachos, us, covid, covid19us, coronavirus]                                                                                                                  \n",
       "2    [humoursnlon, covid, coronavirususa]                                                                                                                                                \n",
       "3    [go, three, diff, convenient, stores, find, real, milk, soy, lower, priced, non, organic, stuff, buy, organic, fine, wow, covid, 19, panic, buying, begins, manhattan]              \n",
       "4    [covid, coronovirius, nursing, home, setting, kirkland, 50, symptoms, 3120]                                                                                                         \n",
       "Name: stop_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stop_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    did the first of several shopping runs though i ended up walking out of cvs without buying anything if the rate of people coughing at me is any indication well all have covid19 by thursday cover your fucking mouths when you cough\n",
       "1    looks like adios muchachos for the us covid covid19us coronavirus                                                                                                                                                                    \n",
       "2    humoursnlon covid coronavirususa                                                                                                                                                                                                     \n",
       "3    had to go to three diff convenient stores to find real milk not soy and they were all out of the lower priced non organic stuff  i buy organic so i was fine with it but wow covid 19 panic buying begins in manhattan               \n",
       "4    covid coronovirius in nursing home setting  kirkland more than 50 with symptoms 3120                                                                                                                                                 \n",
       "Name: token_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row for row in df1['stop_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v = Word2Vec(size=1000, min_count=500, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v.build_vocab(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('right', 0.09585417807102203),\n",
       " ('twitter', 0.08774752914905548),\n",
       " ('president', 0.07988424599170685),\n",
       " ('waiting', 0.07965778559446335),\n",
       " ('chinese', 0.07624496519565582),\n",
       " ('safety', 0.07341757416725159),\n",
       " ('able', 0.07280883193016052),\n",
       " ('helping', 0.06696482747793198),\n",
       " ('united', 0.06436911970376968),\n",
       " ('community', 0.06277808547019958)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions found in this cell were created by instructor Noah Christiansen\n",
    "# He is great\n",
    "# Define vectorization function\n",
    "#word2vecmodel = tweet_w2v\n",
    "def vectorize_corpus(keyword_list):    \n",
    "    # Instantiate counter for number of words in keyword_list that exists\n",
    "    n_words = 0\n",
    "    # Create template for cumulative corpus vector sum\n",
    "    corpus_vec_sum = np.zeros((1,1000))                 \n",
    "    # Scan through each word in list\n",
    "    for word in keyword_list:\n",
    "        if word in tweet_w2v.wv.vocab:                    \n",
    "            word_vec = tweet_w2v.wv.word_vec(word)\n",
    "            #print(word_vec)\n",
    "            n_words +=1                                \n",
    "            corpus_vec_sum = corpus_vec_sum + word_vec \n",
    "    # Compute average vector by taking cumulative vector sum and dividing it by number of words traced\n",
    "    corpus_avg_vec = corpus_vec_sum/n_words\n",
    "    # Squeeze this N-dimensional nested array object into a 1-D array to streamline future processing\n",
    "    corpus_avg_vec = np.squeeze(corpus_avg_vec)\n",
    "    return(corpus_avg_vec)\n",
    "\n",
    "#defining cosine similarity function\n",
    "def cos_sim(vector_1, vector_2):\n",
    "    dp = np.dot(vector_1, vector_2)\n",
    "    magnitude_v1 = np.sqrt(np.dot(vector_1,vector_1))\n",
    "    magnitude_v2 = np.sqrt(np.dot(vector_2,vector_2))\n",
    "    return(dp/(magnitude_v1*magnitude_v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "vect_tweets = [vectorize_corpus(tweet) for tweet in df['stop_text']]\n",
    "vect_df = pd.DataFrame(vect_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(vect_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = df.join(vect_df)\n",
    "join_df.dropna(inplace = True)\n",
    "join_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vec_df= join_df.iloc[:, [i for i in range(13,1012)]]\n",
    "clean_og_df = join_df.iloc[:, [i for i in range(0,12)]]\n",
    "clean_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_og_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(metric='cosine', eps=0.4, min_samples=30, n_jobs = -1) # you can change these parameters, given just for example \n",
    "cluster_labels = dbscan.fit_predict(data) # where X - is your matrix, where each row corresponds to one document (line) from the docs, you need to cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_og_df['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_og_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_og_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = clean_og_df['cluster'] == 0\n",
    "clean_og_df[mask].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
