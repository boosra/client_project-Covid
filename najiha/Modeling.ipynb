{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Import Libraries](#Import-Libraries)**.  \n",
    "    \n",
    "- **[Data Cleaning & EDA](#Data-Cleaning-Exploratory-Data-Analysis)**.  \n",
    "\n",
    "- **[Model Preparation](#Model-Preparation)**. \n",
    "\n",
    " - **[Modeling](#Modeling)**.  \n",
    "   - **[Baseline Model](#Baseline-Model)**. \n",
    "   - **[DBSCAN](#DBSCAN)**.  \n",
    "   - **[K-Means](#K-Means)**.  \n",
    "   \n",
    "- **[Model Evaluation](#Model-Evaluation)**.  \n",
    "\n",
    "- **[Conclusions and Recommendations](#Conclusions-and-Recommendations)**.  \n",
    "\n",
    "- **[References](#References)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN , KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.datasets import make_blobs\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "RANDOM_STATE = 7777\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>search_term</th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>radius</th>\n",
       "      <th>query_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228415289269858310</td>\n",
       "      <td>Bearjew1964</td>\n",
       "      <td>Disease modelers gaze into their computers to ...</td>\n",
       "      <td>2020-02-14 20:26:42</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>40.650002</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>10mi</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1228345808954765313</td>\n",
       "      <td>Carlos Salazar</td>\n",
       "      <td>I know we have a limited attention span and al...</td>\n",
       "      <td>2020-02-14 15:50:37</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>40.650002</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>10mi</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228149699145871360</td>\n",
       "      <td>Hefe O-Ren Ishii</td>\n",
       "      <td>As one Princess cruise is quarantined for COVI...</td>\n",
       "      <td>2020-02-14 02:51:20</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>40.650002</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>10mi</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1228008158586720256</td>\n",
       "      <td>josh</td>\n",
       "      <td>how about we call it by it’s new name: COVID-1...</td>\n",
       "      <td>2020-02-13 17:28:54</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>40.650002</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>10mi</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1227684262859636738</td>\n",
       "      <td>Helen Ong</td>\n",
       "      <td>Scorpio: To me, the #COVID_19 19, as virulent ...</td>\n",
       "      <td>2020-02-12 20:01:52</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>40.650002</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>10mi</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id          username  \\\n",
       "0  1228415289269858310       Bearjew1964   \n",
       "1  1228345808954765313    Carlos Salazar   \n",
       "2  1228149699145871360  Hefe O-Ren Ishii   \n",
       "3  1228008158586720256              josh   \n",
       "4  1227684262859636738         Helen Ong   \n",
       "\n",
       "                                                text           tweet_date  \\\n",
       "0  Disease modelers gaze into their computers to ...  2020-02-14 20:26:42   \n",
       "1  I know we have a limited attention span and al...  2020-02-14 15:50:37   \n",
       "2  As one Princess cruise is quarantined for COVI...  2020-02-14 02:51:20   \n",
       "3  how about we call it by it’s new name: COVID-1...  2020-02-13 17:28:54   \n",
       "4  Scorpio: To me, the #COVID_19 19, as virulent ...  2020-02-12 20:01:52   \n",
       "\n",
       "  search_term      city        lat       long radius query_start  \n",
       "0       COVID  Brooklyn  40.650002 -73.949997   10mi  2020-02-01  \n",
       "1       COVID  Brooklyn  40.650002 -73.949997   10mi  2020-02-01  \n",
       "2       COVID  Brooklyn  40.650002 -73.949997   10mi  2020-02-01  \n",
       "3       COVID  Brooklyn  40.650002 -73.949997   10mi  2020-02-01  \n",
       "4       COVID  Brooklyn  40.650002 -73.949997   10mi  2020-02-01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../adam/datasets/scrape_5.11.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96507, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "df['token_text'] = df['text'].str.replace('([^ a-zA-Z0-9])', '').str.replace('http\\S+|www.\\S+', '', case=False).replace('coronavirus', 'covid19')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "df['stop_text'] = df['token_text'].apply(lambda x: [item for item in str(x).split() \n",
    "                                                    if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row for row in df['stop_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v = Word2Vec(size=1000, min_count=100, window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v.build_vocab(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('photo', 0.09064854681491852),\n",
       " ('caused', 0.08920827507972717),\n",
       " ('meanwhile', 0.08866623789072037),\n",
       " ('empty', 0.08706696331501007),\n",
       " ('leadership', 0.08595381677150726),\n",
       " ('ran', 0.08412136137485504),\n",
       " ('case', 0.08400179445743561),\n",
       " ('keeping', 0.0833788737654686),\n",
       " ('man', 0.08250793814659119),\n",
       " ('selfish', 0.08079575002193451)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Cleaning & EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id       0\n",
       "username       1\n",
       "text           0\n",
       "tweet_date     0\n",
       "search_term    0\n",
       "city           0\n",
       "lat            0\n",
       "long           0\n",
       "radius         0\n",
       "query_start    0\n",
       "token_text     0\n",
       "stop_text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() #droping null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96506, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale our data \n",
    "ss = StandardScaler()\n",
    "\n",
    "#fit and transform Data\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate DummyClassifier\n",
    "# dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "# dummy.fit(X_train, y_train)\n",
    "\n",
    "# # score on test\n",
    "# print('Test Score:', dummy.score(X_test, y_test))\n",
    "\n",
    "# # score on train\n",
    "# print('Train Score:', dummy.score(X_train, y_train))\n",
    "\n",
    "# # score on cross val\n",
    "# print('Cross Val Score:', cross_val_score(dummy, X, y, cv =5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate PCA & logisticRegression\n",
    "# pca = PCA(random_state=RANDOM_STATE)\n",
    "# lr_pca = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "# # Get PC's by applying transformer on data\n",
    "# Z_train = pca.fit_transform(X_train_ss)\n",
    "# Z_test  = pca.transform(X_test_ss)\n",
    "\n",
    "# #fitting model\n",
    "# lr_pca.fit(Z_train, y_train)\n",
    "\n",
    "# print('lr_pca Score:',lr_pca.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling \n",
    "ss = StandardScaler()\n",
    "X_scaled = ss.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = [.5, .65, .8],\n",
    "               min_samples= [5, 10, 20]\n",
    "               ) \n",
    "\n",
    "dbscan.fit(X_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhoutte Score : \", silhouette_score(X_scaled, dbscan.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a k-means clustering model\n",
    "km = KMeans(n_clusters= [3,5,10,15], \n",
    "            n_init= [10, 20, 30],\n",
    "            max_iter= [300, 400, 500],\n",
    "            random_state=2020)\n",
    "#fitting model\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of squared errors for each cluster.\n",
    "print(\"Inertia: \", km.inertia_)\n",
    "\n",
    "#Silhouette Score\n",
    "print(\"Silhouette Score: \", silhouette_score(X_scaled, km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.base import BaseEstimator\n",
    "\n",
    "# class KNNRegressor(BaseEstimator):\n",
    "#     def __init__(self, k):\n",
    "#         self.k = k\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f\"KNNRegressor {{k = {self.k}}}\"\n",
    "    \n",
    "#     def euclidian_distance(self, a, b):\n",
    "#         return np.sqrt(np.sum((a - b)**2))\n",
    "    \n",
    "#     def get_k_nearest_neigbhors_indices(self, distances):\n",
    "#         # create a list of tuples with (distance, index) - HINT: try enumerate\n",
    "#         distances_and_indices = [(distance, index) for index, distance in enumerate(distances)]\n",
    "\n",
    "#         # sort distances with indices using .sort()\n",
    "#         distances_and_indices.sort()\n",
    "\n",
    "#         # select first k distances\n",
    "#         k_distances_and_indices = distances_and_indices[:self.k]\n",
    "\n",
    "#         # return a list of just the indices from first k distances\n",
    "#         return [index for distance, index in k_distances_and_indices]\n",
    "\n",
    "#     def predict_row(self, test_row):\n",
    "#         # Use the `euclidian_distance` function create a list of distances \n",
    "#         # between the `test_row` and each row in `train_data`\n",
    "#         distances = [self.euclidian_distance(row, test_row) for row in self.X_train_]\n",
    "\n",
    "#         # call the `get_k_nearest_neigbhors_indices` function with the distances from the previous step and `k`\n",
    "#         k_distances_and_indices = self.get_k_nearest_neigbhors_indices(distances)\n",
    "\n",
    "#         # use the `k_distances_and_indices` from the previous step to index the `train_target`\n",
    "#         k_nearest_neigbhor_targets = self.y_train_[k_distances_and_indices]\n",
    "\n",
    "#         # return the mean of `k_nearest_neigbhor_targets` from the previous step\n",
    "#         return np.mean(k_nearest_neigbhor_targets)\n",
    "    \n",
    "#     def r2_score(self, y_true, y_pred):\n",
    "#         ### Sum of Squared Errors from Resisuals\n",
    "#         residuals = y_true - y_pred\n",
    "#         ss_residual = np.sum(residuals ** 2)\n",
    "\n",
    "#         ### Sum of Squared Errors from baseline model (predicting mean)\n",
    "#         mean = np.mean(y_true)\n",
    "#         ss_total = np.sum((y_true - mean) ** 2)\n",
    "\n",
    "#         if ss_total == 0: # can't divide by 0\n",
    "#             print(\"Can't divide by 0\")\n",
    "#             return 0.0\n",
    "\n",
    "#         return 1 - (ss_residual / ss_total)\n",
    "    \n",
    "#     def fit(self, X_train, y_train):\n",
    "#         self.X_train_ = X_train\n",
    "#         self.y_train_ = y_train\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X_test):\n",
    "#         preds = [self.predict_row(test_row) for test_row in X_test]\n",
    "#         return np.array(preds)\n",
    "    \n",
    "#     def score(self, X, y_true):\n",
    "#         y_preds = self.predict(X)\n",
    "#         return self.r2_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNRegressor(k = 5)\n",
    "knn.fit(ss_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train score\n",
    "print(\"Train score: {:.2%}\".format(knn.score(ss_X_train, y_train)))\n",
    "\n",
    "# test score\n",
    "print(\"Train score: {:.2%}\".format(knn.score(ss_X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross val score\n",
    "cross_val_score(knn, X, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
