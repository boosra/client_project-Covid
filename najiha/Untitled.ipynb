{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitter_scraper\n",
      "  Downloading twitter_scraper-0.4.1-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting MechanicalSoup\n",
      "  Downloading MechanicalSoup-0.12.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from MechanicalSoup->twitter_scraper) (4.5.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from MechanicalSoup->twitter_scraper) (4.8.2)\n",
      "Requirement already satisfied: six>=1.4 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from MechanicalSoup->twitter_scraper) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from MechanicalSoup->twitter_scraper) (2.22.0)\n",
      "Collecting parse\n",
      "  Downloading parse-1.15.0.tar.gz (29 kB)\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-0.0.25.tar.gz (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fake-useragent\n",
      "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.1-py2.py3-none-any.whl (22 kB)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-1.21.0-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4>=4.4->MechanicalSoup->twitter_scraper) (1.9.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->MechanicalSoup->twitter_scraper) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->MechanicalSoup->twitter_scraper) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->MechanicalSoup->twitter_scraper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->MechanicalSoup->twitter_scraper) (2019.11.28)\n",
      "Collecting pyee\n",
      "  Downloading pyee-7.0.1-py2.py3-none-any.whl (12 kB)\n",
      "Collecting websockets\n",
      "  Downloading websockets-8.1-cp37-cp37m-macosx_10_6_intel.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 8.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting appdirs\n",
      "  Downloading appdirs-1.4.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from pyppeteer>=0.0.14->requests-html->twitter_scraper) (4.42.1)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: parse, pyppeteer, fake-useragent, bs4\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parse: filename=parse-1.15.0-py3-none-any.whl size=23709 sha256=90ce9690d5e1560f86b209ee765414916330ed434121e8ddce426f2a17effc80\n",
      "  Stored in directory: /Users/najihafatemaboosra/Library/Caches/pip/wheels/d7/b3/1d/5c94c64413b2212f64a297c92f11edd45e4474d08d0220a008\n",
      "  Building wheel for pyppeteer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-py3-none-any.whl size=78361 sha256=7c2cd6560d9cadaa006a06769bb248d9fb8a5e74c0148e1da969d29be981e22f\n",
      "  Stored in directory: /Users/najihafatemaboosra/Library/Caches/pip/wheels/ad/4c/02/f888feeabb0c1b751d00a3c2da2e1ccd1e713e72795b864857\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13487 sha256=9d40afbb58369290b01c72b81df82f6f97b0583a957cef303e82b26017f5243c\n",
      "  Stored in directory: /Users/najihafatemaboosra/Library/Caches/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=25733f640231ec9a5c5cf64f6231539ff52127f45d15faaa0b88a4ce3a560e97\n",
      "  Stored in directory: /Users/najihafatemaboosra/Library/Caches/pip/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
      "Successfully built parse pyppeteer fake-useragent bs4\n",
      "Installing collected packages: MechanicalSoup, parse, pyee, websockets, appdirs, pyppeteer, fake-useragent, bs4, cssselect, pyquery, w3lib, requests-html, twitter-scraper\n",
      "Successfully installed MechanicalSoup-0.12.0 appdirs-1.4.3 bs4-0.0.1 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.15.0 pyee-7.0.1 pyppeteer-0.0.25 pyquery-1.4.1 requests-html-0.10.0 twitter-scraper-0.4.1 w3lib-1.21.0 websockets-8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install twitter_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TweetScraper in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (1.2.6)\n",
      "Requirement already satisfied: scrapy in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from TweetScraper) (2.1.0)\n",
      "Requirement already satisfied: pymongo in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from TweetScraper) (3.10.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (1.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (5.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (0.1.16)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (1.5.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (4.5.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (2.0.5)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (18.1.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (2.8)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (1.5.2)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (19.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (20.3.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from scrapy->TweetScraper) (1.21.0)\n",
      "Requirement already satisfied: setuptools in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy->TweetScraper) (46.0.0.post20200309)\n",
      "Requirement already satisfied: six in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from protego>=0.1.15->scrapy->TweetScraper) (1.14.0)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy->TweetScraper) (0.2.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy->TweetScraper) (19.3.0)\n",
      "Requirement already satisfied: pyasn1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy->TweetScraper) (0.4.8)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy->TweetScraper) (1.14.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy->TweetScraper) (17.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy->TweetScraper) (20.2.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy->TweetScraper) (19.0.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy->TweetScraper) (2.0.2)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy->TweetScraper) (15.1.0)\n",
      "Requirement already satisfied: pycparser in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy->TweetScraper) (2.19)\n",
      "Requirement already satisfied: idna>=2.5 in /Users/najihafatemaboosra/opt/anaconda3/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy->TweetScraper) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install TweetScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from twitter_scraper import get_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMs are fine. She just hasn’t responded.\n",
      "weird dream\n",
      "“shared a tweet with you” >>>>>\n",
      "who's up?\n",
      "Ahhhhfdsjkhfjskhfjkldshfkljsdhfkjdhfkjlshdfkljsdhfjkdshfjksdhfjdksfhjkdshfjsdhfjkshfjksdfhjksdhfjksdfhjkdsfh\n",
      "\n",
      "your turn.\n",
      "You need a haircut. \n",
      "Send us a selfie.\n",
      "It's Tuesday\n",
      "Twitter https://twitter.com/chancetherapper/status/1250133538525102083 …\n",
      "Stretchhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "Send pet pics\n",
      "Tagging some friends to help spread the gratitude @shakira @MariahCarey @Zionwilliamson @rickygervais @liamgallagher @SofiaVergara @CP3 @MeekMill @samsmith @De11eDonne @TheEllenShow @mPinoe\n",
      "Tweet some  for all the amazing healthcare workers around the world.\n",
      "\n",
      " #WorldHealthDay\n",
      "For the latest information, follow the @WHO on Twitter and this list of official local health organizations around the world. \n",
      "https://twitter.com/i/lists/1235273855331176448 …\n",
      "What are some good mental health tips? https://twitter.com/WHO/status/1247538430373793793 …\n",
      "When should we expect a vaccine? https://twitter.com/WHO/status/1247538237565894660 …\n",
      "Are face masks effective? https://twitter.com/WHO/status/1247530375766663170 …\n",
      "Can normal laundry washing get rid of #COVID19? https://twitter.com/WHO/status/1247493431292805120 …\n",
      "Will #COVID19 die down during the warmer months? https://twitter.com/WHO/status/1247491892163608576 …\n",
      "What supplies should everyone have on hand if areas go into lockdown? https://twitter.com/WHO/status/1247536386149773314 …\n",
      "Is it ok to exercise outside? https://twitter.com/WHO/status/1247489501582241792 …\n"
     ]
    }
   ],
   "source": [
    "for tweet in get_tweets('twitter', pages=1):\n",
    "        print(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "# \t'tweetId': '1255298173729419264',\n",
    "# \t'isRetweet': False,\n",
    "# \t'time': datetime.datetime(2020, 4, 28, 20, 49, 41),\n",
    "# \t'text': 'DMs are fine. She just hasn’t responded.',\n",
    "# \t'replies': 22167,\n",
    "# \t'retweets': 195561,\n",
    "# \t'likes': 859762,\n",
    "# \t'entries': {\n",
    "# \t\t'hashtags': [],\n",
    "# \t\t'urls': [],\n",
    "# \t\t'photos': [],\n",
    "# \t\t'videos': []\n",
    "# \t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_scraper import get_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#NationalNursesDay',\n",
       " '#ThankYouNurses',\n",
       " 'Grandma Killer',\n",
       " '#JusticeForAhmaud',\n",
       " 'Bethany',\n",
       " 'Retro Blue',\n",
       " '#Chromatica',\n",
       " 'Jerry Rice',\n",
       " '#NursesDay2020',\n",
       " 'New Blacks']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_scraper import Profile\n",
    "profile = Profile('Covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'CoviD P. NA',\n",
       " 'username': 'Covid',\n",
       " 'birthday': None,\n",
       " 'biography': '',\n",
       " 'website': '',\n",
       " 'profile_photo': 'https://pbs.twimg.com/profile_images/475588180/CD_COVER_copy234-001_400x400.jpg',\n",
       " 'likes_count': 8,\n",
       " 'tweets_count': 19,\n",
       " 'followers_count': 10,\n",
       " 'following_count': 16}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " profile.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# tweets = []\n",
    "# for file in os.listdir('tweet/'):\n",
    "#     filename = 'tweet/' + str(file)\n",
    "#     if filename[7:10].isdigit():\n",
    "#         with open(filename) as tweetfile:\n",
    "#             pyresponse = json.loads(tweetfile.read())\n",
    "#             tweets.append(pyresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(tweets)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryMaker:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.query = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "count = 0\n",
    "for tweet in query_tweets(\"I have Covid19, geocode:40.714353,-74.00597299999998,20km\",begindate=datetime.date(2019,12,1)):\n",
    "    chirp = {}\n",
    "    chirp['Tweet_ID'] = tweet.tweet_id\n",
    "    chirp['Username'] = tweet.username\n",
    "    chirp['Text'] = tweet.text\n",
    "    \n",
    "    tweets.update({count:chirp})\n",
    "    count +=1\n",
    "\n",
    "df = pd.DataFrame.from_dict(tweets, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Search_Term'] = '#Covid19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/covid19Tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/covid19Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitterscraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-714823c55cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[0;31m#Library module for .csv file check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitterscraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquery_tweets\u001b[0m \u001b[0;31m#if you haven't installed this module, run 'pip install twitterscraper' in your notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#---------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m query_list = [ #This is our sample list, add or subtract as you see fit!\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitterscraper'"
     ]
    }
   ],
   "source": [
    "#Import requisite modules\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os #Library module for .csv file check\n",
    "from twitterscraper import query_tweets #if you haven't installed this module, run 'pip install twitterscraper' in your notebook\n",
    "#---------------------------------------------------------------------\n",
    "query_list = [ #This is our sample list, add or subtract as you see fit!\n",
    "    'COVID',\n",
    "    'COVID-19',\n",
    "    'Corona',\n",
    "    'Coronavirus',\n",
    "    'Rona',\n",
    "    'Quarantine',\n",
    "    '#COVID',\n",
    "    '#COVID-19',\n",
    "    '#quarantine',\n",
    "    '#Quarantine',\n",
    "    '#covid19'\n",
    "]\n",
    "#--------------------------------------------------------------------\n",
    "#Credit to Danielle Medellin, DSI11-NYC for the below implementation of custom parameter dictionary support\n",
    "custom_params = {'Houston':{'lat'  : 29.760427,\n",
    "                         'long' : -95.369804,\n",
    "                         'radius': '15mi',\n",
    "                         'queries' : ['rona','corona','covid']},\n",
    "              'Detroit': {'city' : 'Detroit',\n",
    "                          'lat'  : 42.331429,\n",
    "                          'long' : -83.045753,\n",
    "                          'radius' : '10mi',\n",
    "                         'queries': ['stonks','tom nook','animal crossing']}\n",
    "             }\n",
    "#--------------------------------------------------------------------\n",
    "#Get tweets without geolocation\n",
    "def get_tweets(query): \n",
    "    tweets = {}\n",
    "    count = 0 #Sets the index generator\n",
    "    for tweet in query_tweets(query,begindate=datetime.date(2019,12,1)):\n",
    "        chirp = {}\n",
    "        chirp['tweet_id'] = tweet.tweet_id\n",
    "        chirp['username'] = tweet.username\n",
    "        chirp['text'] = tweet.text\n",
    "        chirp['tweet_date'] = tweet.timestamp\n",
    "        chirp['search_term'] = query\n",
    "        chirp['city'] = np.NaN #Fills columns with NaNs for data cleaning at a later point. Rather than having to replace string values\n",
    "        chirp['lat'] = np.NaN #These values can be replaced with fillna.\n",
    "        chirp['long'] = np.NaN\n",
    "        chirp['radius'] = np.NaN\n",
    "        tweets.update({count : chirp})\n",
    "        count += 1\n",
    "    return tweets\n",
    "#--------------------------------------------------------------------\n",
    "#Get tweets with geolocation\n",
    "def get_tweets_geoloc(query, city, lat, long, radius): #Geolocation parameters defined by user in master function or dictionary\n",
    "    tweets = {}\n",
    "    count = 0\n",
    "    for tweet in query_tweets(f\"{query}, geocode:{lat},{long},{radius}\",begindate=datetime.date(2019,12,1)):\n",
    "        chirp = {} #Generates tweet dictionary by calling on generated 'tweet' object attributes\n",
    "        chirp['tweet_id'] = tweet.tweet_id\n",
    "        chirp['username'] = tweet.username\n",
    "        chirp['text'] = tweet.text\n",
    "        chirp['tweet_date'] = tweet.timestamp\n",
    "        chirp['search_term'] = query\n",
    "        chirp['city'] = city\n",
    "        chirp['lat'] = lat\n",
    "        chirp['long'] = long\n",
    "        chirp['radius'] = radius\n",
    "        tweets.update({count : chirp})\n",
    "        count += 1 #increments index up by 1 for later dataframe implementation\n",
    "    return tweets\n",
    "#--------------------------------------------------------------------\n",
    "#Generate dataframe from \"tweets\" dictionary generated after each query\n",
    "def make_dataframe(dictionary):\n",
    "    df = pd.DataFrame.from_dict(dictionary, orient='index') \n",
    "    return df\n",
    "#--------------------------------------------------------------------\n",
    "#Query function using custom parameters\n",
    "#Credit Danielle Medellin for this code block section\n",
    "def get_query_dataframe_cp(custom_params):\n",
    "    query_df = pd.DataFrame() #instantiate an empty dataframe\n",
    "    for key in custom_params.keys(): #Generates a new query dataframe for each city used in the \n",
    "        for query in custom_params[key]['queries']: #Runs a unique query for each unique term in the query key\n",
    "            tweets = get_tweets_geoloc(query,custom_params[key],custom_params[key]['lat'],custom_params[key]['long'],custom_params[key]['radius'])\n",
    "            df = make_dataframe(tweets)\n",
    "            query_df = pd.concat([query_df,df],ignore_index = True)\n",
    "    return query_df\n",
    "#---------------------------------------------------------------\n",
    "#Query function with geolocation but no custom parameters\n",
    "def get_query_dataframe_geo(list_of_queries,city,lat,long,radius):\n",
    "    query_df = pd.DataFrame() #instantiate an empty dataframe\n",
    "    for query in list_of_queries:\n",
    "            tweets = get_tweets_geoloc(query,city,lat,long,radius)\n",
    "            df = make_dataframe(tweets)\n",
    "            query_df = pd.concat([query_df,df],ignore_index = True)\n",
    "    return query_df\n",
    "#-------------------------------------------------------------------\n",
    "#Query function with no custom anything\n",
    "def get_query_dataframe(list_of_queries):\n",
    "    query_df = pd.DataFrame() #instantiate an empty dataframe\n",
    "    for query in list_of_queries:\n",
    "            tweets = get_tweets(query)\n",
    "            df = make_dataframe(tweets)\n",
    "            query_df = pd.concat([query_df,df],ignore_index = True)\n",
    "    return query_df\n",
    "#------------------------------------------------------------------\n",
    "#Master function\n",
    "def get_dataset():\n",
    "    #Paramter switches\n",
    "    custom_params_switch = input(\"Are you using a custom parameter dictionary?\")\n",
    "    export_csv_switch = input(\"Do you want to export the final dataframe to csv?\")\n",
    "    #Custom parameter switch block\n",
    "    if str.lower(custom_params_switch) == 'yes':\n",
    "        dataset = get_query_dataframe_cp(custom_params)\n",
    "    else:\n",
    "        geo_switch = input(\"Are you using geolocation?\")\n",
    "        if str.lower(geo_switch) == 'yes':\n",
    "            lat = float(input(\"Input Latitude:\")) #Converts string input latitude to float value\n",
    "            long = float(input(\"Input Longitude:\"))\n",
    "            city = input(\"Input city or neighborhood corresponding to coordinates:\") #Allows filling of city values\n",
    "            radius = input(\"Input radius and unit:\")\n",
    "            dataset = get_query_dataframe_geo(query_list, city, lat, long, radius)\n",
    "        else:\n",
    "            dataset = get_query_dataframe(query_list)\n",
    "    #CSV export switch block\n",
    "    if str.lower(export_csv_switch) == 'yes':\n",
    "        custom_csv_name = input(\"Input CSV export file name:\") #user input line for export name\n",
    "        #Check if datasets folder exists. If not, create folder.\n",
    "        if os.path.exists('datasets') == True:\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir('datasets')\n",
    "        #Check if file has already been created. If yes, prompt user to overwrite or make new file.\n",
    "        if os.path.exists(f'datasets/{custom_csv_name}.csv') == True:\n",
    "            overwrite_check = input (\"File already exists--do you want to overwrite?\")\n",
    "            if str.lower(overwrite_check) == 'yes':\n",
    "                pass #skips through checks and overwrites file name\n",
    "            else:\n",
    "                new_csv_name = custom_csv_name #creates new_csv_name variable = to old name\n",
    "                while new_csv_name == custom_csv_name: #continues to reject file name until a unique name is created\n",
    "                    new_csv_name = input(\"Input new output file name:\")\n",
    "                custom_csv_name = new_csv_name #Sets the file name to the new user input\n",
    "        else:\n",
    "            pass\n",
    "        dataset.to_csv(f\"./datasets/{custom_csv_name}.csv\", index = False) #write csv to datasets folder\n",
    "        print(f\"Export complete, scraped {len(dataset.index)} tweets\") #Prints completion statement including total tweets scraped\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
